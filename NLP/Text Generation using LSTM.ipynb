{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7b72bdef-40e2-4ad3-ba69-1def99ed893f",
      "metadata": {
        "id": "7b72bdef-40e2-4ad3-ba69-1def99ed893f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "00bf925d-d262-4276-98d8-32335a488b1d",
      "metadata": {
        "id": "00bf925d-d262-4276-98d8-32335a488b1d"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/shakespeare.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "094dea7c-e6c4-4ce4-9874-1fc0b8c36eaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "094dea7c-e6c4-4ce4-9874-1fc0b8c36eaa",
        "outputId": "bb53eaed-a293-4808-af51-0a7d58ed9684"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "type(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "49534a03-25f3-4bcf-babb-698fe5a2a119",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49534a03-25f3-4bcf-babb-698fe5a2a119",
        "outputId": "04a108a1-671b-4458-fb72-a5678ecaa21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When forty winters shall besiege thy brow,\n",
            "  And dig deep trenches in thy beauty's field,\n",
            "  Thy youth's proud livery so gazed on now,\n",
            "  Will be a tattered weed of small worth held:  \n",
            "  Then being asked, where all thy beauty lies,\n",
            "  Where all the treasure of thy lusty days;\n",
            "  To say within thine own deep su\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b613b4b5-f16f-4929-b471-892e66e9e917",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b613b4b5-f16f-4929-b471-892e66e9e917",
        "outputId": "596d07d1-e130-46c2-9e08-b20830f81723"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2c627f36-abe3-4226-a17e-607bb3f92022",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c627f36-abe3-4226-a17e-607bb3f92022",
        "outputId": "1276186d-3bce-4b10-c5f7-f542642cd871"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "unique_char = set(text)\n",
        "len(unique_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e8e0badf-e053-4075-a8eb-c02271d6cccf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8e0badf-e053-4075-a8eb-c02271d6cccf",
        "outputId": "b4b74a8f-42df-496b-ad80-4f87821f70a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '3')\n",
            "(1, 's')\n",
            "(2, 'V')\n",
            "(3, 'Q')\n",
            "(4, 'R')\n",
            "(5, 'e')\n",
            "(6, 'd')\n",
            "(7, 'H')\n",
            "(8, 'x')\n",
            "(9, 'S')\n",
            "(10, 'A')\n",
            "(11, 'J')\n",
            "(12, 'm')\n",
            "(13, ',')\n",
            "(14, 'P')\n",
            "(15, 'p')\n",
            "(16, 'I')\n",
            "(17, ']')\n",
            "(18, '9')\n",
            "(19, ';')\n",
            "(20, 'k')\n",
            "(21, '1')\n",
            "(22, '_')\n",
            "(23, 't')\n",
            "(24, '<')\n",
            "(25, '(')\n",
            "(26, 'E')\n",
            "(27, 'T')\n",
            "(28, 'u')\n",
            "(29, 'w')\n",
            "(30, 'a')\n",
            "(31, 'M')\n",
            "(32, ')')\n",
            "(33, '2')\n",
            "(34, 'q')\n",
            "(35, 'o')\n",
            "(36, 'B')\n",
            "(37, '-')\n",
            "(38, '8')\n",
            "(39, '0')\n",
            "(40, 'f')\n",
            "(41, '4')\n",
            "(42, '>')\n",
            "(43, ' ')\n",
            "(44, 'Y')\n",
            "(45, '&')\n",
            "(46, 'G')\n",
            "(47, '6')\n",
            "(48, 'K')\n",
            "(49, '\"')\n",
            "(50, ':')\n",
            "(51, '.')\n",
            "(52, 'v')\n",
            "(53, 'l')\n",
            "(54, 'L')\n",
            "(55, 'X')\n",
            "(56, '5')\n",
            "(57, '!')\n",
            "(58, 'y')\n",
            "(59, 'F')\n",
            "(60, 'z')\n",
            "(61, '7')\n",
            "(62, 'Z')\n",
            "(63, 'N')\n",
            "(64, 'j')\n",
            "(65, 'n')\n",
            "(66, 'W')\n",
            "(67, '`')\n",
            "(68, \"'\")\n",
            "(69, 'b')\n",
            "(70, 'C')\n",
            "(71, 'D')\n",
            "(72, '\\n')\n",
            "(73, 'U')\n",
            "(74, 'O')\n",
            "(75, '|')\n",
            "(76, '}')\n",
            "(77, 'c')\n",
            "(78, '?')\n",
            "(79, '[')\n",
            "(80, 'r')\n",
            "(81, 'g')\n",
            "(82, 'h')\n",
            "(83, 'i')\n"
          ]
        }
      ],
      "source": [
        "for pair in enumerate(unique_char):\n",
        "    print(pair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0d963d17-5e1c-416a-8223-881e21cb7cc8",
      "metadata": {
        "id": "0d963d17-5e1c-416a-8223-881e21cb7cc8"
      },
      "outputs": [],
      "source": [
        "# number -> letter\n",
        "decoder = dict(enumerate(unique_char))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6c1323d9-eabc-46ba-baa1-1577f03c2250",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c1323d9-eabc-46ba-baa1-1577f03c2250",
        "outputId": "d8c4d63c-6ec9-46e0-cc24-75bbd31cfa13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'3': 0,\n",
              " 's': 1,\n",
              " 'V': 2,\n",
              " 'Q': 3,\n",
              " 'R': 4,\n",
              " 'e': 5,\n",
              " 'd': 6,\n",
              " 'H': 7,\n",
              " 'x': 8,\n",
              " 'S': 9,\n",
              " 'A': 10,\n",
              " 'J': 11,\n",
              " 'm': 12,\n",
              " ',': 13,\n",
              " 'P': 14,\n",
              " 'p': 15,\n",
              " 'I': 16,\n",
              " ']': 17,\n",
              " '9': 18,\n",
              " ';': 19,\n",
              " 'k': 20,\n",
              " '1': 21,\n",
              " '_': 22,\n",
              " 't': 23,\n",
              " '<': 24,\n",
              " '(': 25,\n",
              " 'E': 26,\n",
              " 'T': 27,\n",
              " 'u': 28,\n",
              " 'w': 29,\n",
              " 'a': 30,\n",
              " 'M': 31,\n",
              " ')': 32,\n",
              " '2': 33,\n",
              " 'q': 34,\n",
              " 'o': 35,\n",
              " 'B': 36,\n",
              " '-': 37,\n",
              " '8': 38,\n",
              " '0': 39,\n",
              " 'f': 40,\n",
              " '4': 41,\n",
              " '>': 42,\n",
              " ' ': 43,\n",
              " 'Y': 44,\n",
              " '&': 45,\n",
              " 'G': 46,\n",
              " '6': 47,\n",
              " 'K': 48,\n",
              " '\"': 49,\n",
              " ':': 50,\n",
              " '.': 51,\n",
              " 'v': 52,\n",
              " 'l': 53,\n",
              " 'L': 54,\n",
              " 'X': 55,\n",
              " '5': 56,\n",
              " '!': 57,\n",
              " 'y': 58,\n",
              " 'F': 59,\n",
              " 'z': 60,\n",
              " '7': 61,\n",
              " 'Z': 62,\n",
              " 'N': 63,\n",
              " 'j': 64,\n",
              " 'n': 65,\n",
              " 'W': 66,\n",
              " '`': 67,\n",
              " \"'\": 68,\n",
              " 'b': 69,\n",
              " 'C': 70,\n",
              " 'D': 71,\n",
              " '\\n': 72,\n",
              " 'U': 73,\n",
              " 'O': 74,\n",
              " '|': 75,\n",
              " '}': 76,\n",
              " 'c': 77,\n",
              " '?': 78,\n",
              " '[': 79,\n",
              " 'r': 80,\n",
              " 'g': 81,\n",
              " 'h': 82,\n",
              " 'i': 83}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# letter -> number\n",
        "encoder = {v: k for k,v in decoder.items()}\n",
        "encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "11551f6b-36dc-418d-952a-3f4be00cdb03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11551f6b-36dc-418d-952a-3f4be00cdb03",
        "outputId": "b2a363e0-9aea-4521-b590-6a82458d8f26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([72, 43, 43, ..., 26, 63, 71])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "encoded_text = np.array([encoder[char] for char in text])\n",
        "encoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c13464b2-6180-4a7a-8813-6205cd1ad5c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c13464b2-6180-4a7a-8813-6205cd1ad5c8",
        "outputId": "12ba4b36-5efd-4461-d4ef-603f151b8955"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([72, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
              "       43, 43, 43, 43, 43, 21, 72, 43, 43, 59, 80, 35, 12, 43, 40, 30, 83,\n",
              "       80,  5,  1, 23, 43, 77, 80,  5, 30, 23, 28, 80,  5,  1, 43, 29,  5,\n",
              "       43,  6,  5,  1, 83, 80,  5, 43, 83, 65, 77, 80,  5, 30,  1,  5, 13,\n",
              "       72, 43, 43, 27, 82, 30, 23, 43, 23, 82,  5, 80,  5, 69, 58, 43, 69,\n",
              "        5, 30, 28, 23, 58, 68,  1, 43, 80, 35,  1,  5, 43, 12, 83, 81, 82,\n",
              "       23, 43, 65,  5, 52,  5, 80, 43,  6, 83,  5, 13, 72, 43, 43, 36, 28,\n",
              "       23, 43, 30,  1, 43, 23, 82,  5, 43, 80, 83, 15,  5, 80, 43,  1, 82,\n",
              "       35, 28, 53,  6, 43, 69, 58, 43, 23, 83, 12,  5, 43,  6,  5, 77,  5,\n",
              "       30,  1,  5, 13, 72, 43, 43,  7, 83,  1, 43, 23,  5, 65,  6,  5, 80,\n",
              "       43, 82,  5, 83, 80, 43, 12, 83, 81, 82, 23, 43, 69,  5, 30, 80, 43,\n",
              "       82, 83,  1, 43, 12,  5, 12, 35, 80, 58, 50, 72, 43, 43, 36, 28, 23,\n",
              "       43, 23, 82, 35, 28, 43, 77, 35, 65, 23, 80, 30, 77, 23,  5,  6, 43,\n",
              "       23, 35, 43, 23, 82, 83, 65,  5, 43, 35, 29, 65, 43, 69, 80, 83, 81,\n",
              "       82, 23, 43,  5, 58,  5,  1, 13, 72, 43, 43, 59,  5,  5,  6, 68,  1,\n",
              "       23, 43, 23, 82, 58, 43, 53, 83, 81, 82, 23, 68,  1, 43, 40, 53, 30,\n",
              "       12,  5, 43, 29, 83, 23, 82, 43,  1,  5, 53, 40, 37,  1, 28, 69,  1,\n",
              "       23, 30, 65, 23, 83, 30, 53, 43, 40, 28,  5, 53, 13, 72, 43, 43, 31,\n",
              "       30, 20, 83, 65, 81, 43, 30, 43, 40, 30, 12, 83, 65,  5, 43, 29, 82,\n",
              "        5, 80,  5, 43, 30, 69, 28, 65,  6, 30, 65, 77,  5, 43, 53, 83,  5,\n",
              "        1, 13, 72, 43, 43, 27, 82, 58, 43,  1,  5, 53, 40, 43, 23, 82, 58,\n",
              "       43, 40, 35,  5, 13, 43, 23, 35, 43, 23, 82, 58, 43,  1, 29,  5,  5,\n",
              "       23, 43,  1,  5, 53, 40, 43, 23, 35, 35, 43, 77, 80, 28,  5, 53, 50,\n",
              "       72, 43, 43, 27, 82, 35, 28, 43, 23, 82, 30, 23, 43, 30, 80, 23, 43,\n",
              "       65, 35, 29, 43, 23, 82,  5, 43, 29, 35, 80, 53,  6, 68,  1, 43, 40,\n",
              "       80,  5,  1, 82, 43, 35, 80, 65, 30, 12,  5, 65, 23, 13, 72, 43, 43,\n",
              "       10, 65,  6, 43, 35, 65, 53, 58, 43, 82,  5, 80, 30, 53,  6, 43, 23,\n",
              "       35, 43, 23, 82,  5, 43, 81, 30, 28,  6, 58, 43,  1, 15, 80, 83, 65,\n",
              "       81, 13, 72, 43, 43, 66, 83, 23, 82, 83, 65, 43, 23, 82, 83, 65,  5,\n",
              "       43, 35, 29, 65, 43, 69, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "encoded_text[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b14e1c2b-6456-48cc-a042-ebf2a506a95d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b14e1c2b-6456-48cc-a042-ebf2a506a95d",
        "outputId": "0109de6c-b4d2-4821-8d56-67fcb562a7bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'d'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "decoder[6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "86ee3464-b238-4718-8cf1-63e6b428c4a8",
      "metadata": {
        "id": "86ee3464-b238-4718-8cf1-63e6b428c4a8"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(encoded_text,num_unique_char):\n",
        "    one_hot = np.zeros((encoded_text.size,num_unique_char))\n",
        "    # Convert to float32 datatype for PyTorch usage later on\n",
        "    one_hot = one_hot.astype(np.float32)\n",
        "    one_hot[np.arange(one_hot.shape[0]),encoded_text.flatten()] = 1.0\n",
        "    one_hot = one_hot.reshape((*encoded_text.shape,num_unique_char))\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a5310535-6cce-4b84-ba1c-b87716a6f9e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5310535-6cce-4b84-ba1c-b87716a6f9e4",
        "outputId": "edb5a05a-8bf4-412e-866f-0cff30cd84ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "arr = np.array([1,2,0])\n",
        "arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d4cda1ed-dce0-4211-b7ba-b2cb942e76b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4cda1ed-dce0-4211-b7ba-b2cb942e76b2",
        "outputId": "52efcebf-a8a3-45e8-b1d2-bc8c9ca423c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "one_hot_encode(arr,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a517c5a0-a56c-4a46-ba5d-0ac6bd7ef3eb",
      "metadata": {
        "id": "a517c5a0-a56c-4a46-ba5d-0ac6bd7ef3eb"
      },
      "outputs": [],
      "source": [
        "def generate_batches(encoded_text,samples_per_batch=10,sequence_length=50):\n",
        "    # X -> encoded text of length sequence length\n",
        "    # [0,1,2]\n",
        "    # [1,2,3]\n",
        "    # Y -> encoded text shifted by 1\n",
        "    # [1,2,3]\n",
        "    # [2,3,4]\n",
        "    # number of characters in each batch\n",
        "    char_per_batch = samples_per_batch * sequence_length\n",
        "    # number of batches given the length of encoded text\n",
        "    num_batches = len(encoded_text) // char_per_batch\n",
        "    # Cut off the end of the encoded text which won't fit into a batch\n",
        "    encoded_text = encoded_text[:num_batches*char_per_batch]\n",
        "    encoded_text = encoded_text.reshape((samples_per_batch,-1))\n",
        "\n",
        "    for n in range(0,encoded_text.shape[1],sequence_length):\n",
        "        x = encoded_text[:,n:n+sequence_length]\n",
        "        y = np.zeros_like(x)\n",
        "\n",
        "        try:\n",
        "            y[:,:-1] = x[:,1:]\n",
        "            y[:,-1] = encoded_text[:,n+sequence_length]\n",
        "        except:\n",
        "            y[:,:-1] = x[:,1:]\n",
        "            y[:,-1] = encoded_text[:,0]\n",
        "\n",
        "        yield x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "28b7714e-f42b-4f1a-842d-34236b2fd327",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28b7714e-f42b-4f1a-842d-34236b2fd327",
        "outputId": "7dc67bf4-0601-486a-c419-62255f8f29a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "sample_text = np.arange(24)\n",
        "sample_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4e88c378-b0f6-45d1-95b4-b3096c728343",
      "metadata": {
        "id": "4e88c378-b0f6-45d1-95b4-b3096c728343"
      },
      "outputs": [],
      "source": [
        "batch_generator = generate_batches(sample_text,samples_per_batch=2,sequence_length=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "66fad260-c86b-4dce-882d-deaddd915497",
      "metadata": {
        "id": "66fad260-c86b-4dce-882d-deaddd915497"
      },
      "outputs": [],
      "source": [
        "x, y = next(batch_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "07cf3019-59c5-4693-a6ff-80dbd804a811",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07cf3019-59c5-4693-a6ff-80dbd804a811",
        "outputId": "69a0bc53-ecee-4d2d-d8ec-a88f19047464"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2,  3,  4],\n",
              "       [10, 11, 12, 13, 14]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6bfc1595-bf12-4b11-b467-33e554af725f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bfc1595-bf12-4b11-b467-33e554af725f",
        "outputId": "6033766b-bce7-4ed3-9a26-3924416fdbed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3,  4,  5],\n",
              "       [11, 12, 13, 14, 15]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e1e785b9-d69b-4907-af4b-85c57904ddaf",
      "metadata": {
        "id": "e1e785b9-d69b-4907-af4b-85c57904ddaf"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self,unique_char,hidden_units=256,num_layers=5,dropout_probability=0.5,use_gpu=False):\n",
        "        super().__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_prob = dropout_probability\n",
        "        self.unique_char = unique_char\n",
        "        self.use_gpu = use_gpu\n",
        "        self.decoder = dict(enumerate(unique_char))\n",
        "        self.encoder = {idx: char for char, idx in self.decoder.items()}\n",
        "        self.lstm = nn.LSTM(len(self.unique_char),hidden_units,num_layers,dropout=dropout_probability,batch_first=True)\n",
        "        self.dropout = nn.Dropout(self.dropout_prob)\n",
        "        self.linear = nn.Linear(self.hidden_units,len(self.unique_char))\n",
        "\n",
        "    def forward(self,seq,hidden):\n",
        "        lstm_output, hidden = self.lstm(seq,hidden)\n",
        "        dropout_output = self.dropout(lstm_output)\n",
        "        dropout_output = dropout_output.contiguous().view((-1,self.hidden_units))\n",
        "        final_output = self.linear(dropout_output)\n",
        "        return final_output, hidden\n",
        "\n",
        "    def hidden_state(self,batch_size):\n",
        "        if self.use_gpu:\n",
        "            return (torch.zeros(self.num_layers,batch_size,self.hidden_units).cuda(),torch.zeros(self.num_layers,batch_size,self.hidden_units).cuda())\n",
        "        else:\n",
        "            return (torch.zeros(self.num_layers,batch_size,self.hidden_units),torch.zeros(self.num_layers,batch_size,self.hidden_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "500cb641-45cc-4877-9378-f36f01005828",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "500cb641-45cc-4877-9378-f36f01005828",
        "outputId": "f17ead0c-f75b-46be-a194-5e3e9d589ce5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (linear): Linear(in_features=512, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model = LSTM(unique_char,hidden_units=512,num_layers=3,dropout_probability=0.5,use_gpu=True)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0be1e623-7b05-4b66-8d9d-9e5ce71a0fd7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0be1e623-7b05-4b66-8d9d-9e5ce71a0fd7",
        "outputId": "ddb0cd68-b1c4-4f72-d781-ed85df43a790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 5470292\n"
          ]
        }
      ],
      "source": [
        "total_params = 0\n",
        "\n",
        "for param in model.parameters():\n",
        "    total_params += param.numel()\n",
        "\n",
        "print(\"Total parameters:\", total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "25a8c9e0-b613-458f-a5af-3b2fc408efdd",
      "metadata": {
        "id": "25a8c9e0-b613-458f-a5af-3b2fc408efdd"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "ca090046-2bea-48a0-91da-d6bcf8b0a3b0",
      "metadata": {
        "id": "ca090046-2bea-48a0-91da-d6bcf8b0a3b0"
      },
      "outputs": [],
      "source": [
        "train_percent = 0.9\n",
        "train_idx = int(len(encoded_text)*train_percent)\n",
        "\n",
        "train_data = encoded_text[:train_idx]\n",
        "test_data = encoded_text[train_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "c14e8bf5-7b20-4884-b2d2-8b1deed4db0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c14e8bf5-7b20-4884-b2d2-8b1deed4db0d",
        "outputId": "f73748ac-e795-4203-e433-ddcd869b220f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Step 25 Validation Loss: 3.19205379486084\n",
            "Epoch 1 Step 50 Validation Loss: 3.179020404815674\n",
            "Epoch 1 Step 75 Validation Loss: 3.059781074523926\n",
            "Epoch 1 Step 100 Validation Loss: 2.9559810161590576\n",
            "Epoch 1 Step 125 Validation Loss: 2.8413662910461426\n",
            "Epoch 1 Step 150 Validation Loss: 2.716247081756592\n",
            "Epoch 1 Step 175 Validation Loss: 2.63789439201355\n",
            "Epoch 1 Step 200 Validation Loss: 2.545100450515747\n",
            "Epoch 1 Step 225 Validation Loss: 2.4303112030029297\n",
            "Epoch 1 Step 250 Validation Loss: 2.340391159057617\n",
            "Epoch 1 Step 275 Validation Loss: 2.2656970024108887\n",
            "Epoch 1 Step 300 Validation Loss: 2.194387197494507\n",
            "Epoch 1 Step 325 Validation Loss: 2.145447015762329\n",
            "Epoch 1 Step 350 Validation Loss: 2.0970492362976074\n",
            "Epoch 1 Step 375 Validation Loss: 2.060091257095337\n",
            "Epoch 1 Step 400 Validation Loss: 2.028076648712158\n",
            "Epoch 1 Step 425 Validation Loss: 1.9985202550888062\n",
            "Epoch 1 Step 450 Validation Loss: 1.9719905853271484\n",
            "Epoch 1 Step 475 Validation Loss: 1.9422369003295898\n",
            "Epoch 2 Step 500 Validation Loss: 1.9190882444381714\n",
            "Epoch 2 Step 525 Validation Loss: 1.8991539478302002\n",
            "Epoch 2 Step 550 Validation Loss: 1.8753163814544678\n",
            "Epoch 2 Step 575 Validation Loss: 1.8579744100570679\n",
            "Epoch 2 Step 600 Validation Loss: 1.8403476476669312\n",
            "Epoch 2 Step 625 Validation Loss: 1.8196017742156982\n",
            "Epoch 2 Step 650 Validation Loss: 1.803218960762024\n",
            "Epoch 2 Step 675 Validation Loss: 1.7873953580856323\n",
            "Epoch 2 Step 700 Validation Loss: 1.7659116983413696\n",
            "Epoch 2 Step 725 Validation Loss: 1.7564103603363037\n",
            "Epoch 2 Step 750 Validation Loss: 1.7392568588256836\n",
            "Epoch 2 Step 775 Validation Loss: 1.7261935472488403\n",
            "Epoch 2 Step 800 Validation Loss: 1.7150481939315796\n",
            "Epoch 2 Step 825 Validation Loss: 1.6997966766357422\n",
            "Epoch 2 Step 850 Validation Loss: 1.693399429321289\n",
            "Epoch 2 Step 875 Validation Loss: 1.6864861249923706\n",
            "Epoch 2 Step 900 Validation Loss: 1.673117995262146\n",
            "Epoch 2 Step 925 Validation Loss: 1.661961317062378\n",
            "Epoch 2 Step 950 Validation Loss: 1.6544902324676514\n",
            "Epoch 2 Step 975 Validation Loss: 1.6369781494140625\n",
            "Epoch 3 Step 1000 Validation Loss: 1.6307393312454224\n",
            "Epoch 3 Step 1025 Validation Loss: 1.6297223567962646\n",
            "Epoch 3 Step 1050 Validation Loss: 1.616894006729126\n",
            "Epoch 3 Step 1075 Validation Loss: 1.6171060800552368\n",
            "Epoch 3 Step 1100 Validation Loss: 1.6042661666870117\n",
            "Epoch 3 Step 1125 Validation Loss: 1.5924588441848755\n",
            "Epoch 3 Step 1150 Validation Loss: 1.5861473083496094\n",
            "Epoch 3 Step 1175 Validation Loss: 1.5760446786880493\n",
            "Epoch 3 Step 1200 Validation Loss: 1.5651172399520874\n",
            "Epoch 3 Step 1225 Validation Loss: 1.5622074604034424\n",
            "Epoch 3 Step 1250 Validation Loss: 1.5535863637924194\n",
            "Epoch 3 Step 1275 Validation Loss: 1.547553300857544\n",
            "Epoch 3 Step 1300 Validation Loss: 1.54420006275177\n",
            "Epoch 3 Step 1325 Validation Loss: 1.5394257307052612\n",
            "Epoch 3 Step 1350 Validation Loss: 1.5336517095565796\n",
            "Epoch 3 Step 1375 Validation Loss: 1.5336540937423706\n",
            "Epoch 3 Step 1400 Validation Loss: 1.5256338119506836\n",
            "Epoch 3 Step 1425 Validation Loss: 1.5188144445419312\n",
            "Epoch 3 Step 1450 Validation Loss: 1.5113482475280762\n",
            "Epoch 4 Step 1475 Validation Loss: 1.514788269996643\n",
            "Epoch 4 Step 1500 Validation Loss: 1.503860592842102\n",
            "Epoch 4 Step 1525 Validation Loss: 1.50137460231781\n",
            "Epoch 4 Step 1550 Validation Loss: 1.4968050718307495\n",
            "Epoch 4 Step 1575 Validation Loss: 1.4964407682418823\n",
            "Epoch 4 Step 1600 Validation Loss: 1.4859601259231567\n",
            "Epoch 4 Step 1625 Validation Loss: 1.4820834398269653\n",
            "Epoch 4 Step 1650 Validation Loss: 1.480337381362915\n",
            "Epoch 4 Step 1675 Validation Loss: 1.4759503602981567\n",
            "Epoch 4 Step 1700 Validation Loss: 1.4652810096740723\n",
            "Epoch 4 Step 1725 Validation Loss: 1.4642850160598755\n",
            "Epoch 4 Step 1750 Validation Loss: 1.4630764722824097\n",
            "Epoch 4 Step 1775 Validation Loss: 1.4641668796539307\n",
            "Epoch 4 Step 1800 Validation Loss: 1.4584801197052002\n",
            "Epoch 4 Step 1825 Validation Loss: 1.4541330337524414\n",
            "Epoch 4 Step 1850 Validation Loss: 1.4589407444000244\n",
            "Epoch 4 Step 1875 Validation Loss: 1.4542062282562256\n",
            "Epoch 4 Step 1900 Validation Loss: 1.4486199617385864\n",
            "Epoch 4 Step 1925 Validation Loss: 1.4468649625778198\n",
            "Epoch 4 Step 1950 Validation Loss: 1.4402319192886353\n",
            "Epoch 5 Step 1975 Validation Loss: 1.4439916610717773\n",
            "Epoch 5 Step 2000 Validation Loss: 1.4426558017730713\n",
            "Epoch 5 Step 2025 Validation Loss: 1.4304498434066772\n",
            "Epoch 5 Step 2050 Validation Loss: 1.430511236190796\n",
            "Epoch 5 Step 2075 Validation Loss: 1.4374332427978516\n",
            "Epoch 5 Step 2100 Validation Loss: 1.4298120737075806\n",
            "Epoch 5 Step 2125 Validation Loss: 1.4252445697784424\n",
            "Epoch 5 Step 2150 Validation Loss: 1.4170997142791748\n",
            "Epoch 5 Step 2175 Validation Loss: 1.417538046836853\n",
            "Epoch 5 Step 2200 Validation Loss: 1.417678952217102\n",
            "Epoch 5 Step 2225 Validation Loss: 1.416703462600708\n",
            "Epoch 5 Step 2250 Validation Loss: 1.4171106815338135\n",
            "Epoch 5 Step 2275 Validation Loss: 1.415462613105774\n",
            "Epoch 5 Step 2300 Validation Loss: 1.412304162979126\n",
            "Epoch 5 Step 2325 Validation Loss: 1.4133538007736206\n",
            "Epoch 5 Step 2350 Validation Loss: 1.41201913356781\n",
            "Epoch 5 Step 2375 Validation Loss: 1.4078224897384644\n",
            "Epoch 5 Step 2400 Validation Loss: 1.4046876430511475\n",
            "Epoch 5 Step 2425 Validation Loss: 1.4022294282913208\n",
            "Epoch 5 Step 2450 Validation Loss: 1.4018573760986328\n",
            "Epoch 6 Step 2475 Validation Loss: 1.4016724824905396\n",
            "Epoch 6 Step 2500 Validation Loss: 1.3990720510482788\n",
            "Epoch 6 Step 2525 Validation Loss: 1.3895602226257324\n",
            "Epoch 6 Step 2550 Validation Loss: 1.3947341442108154\n",
            "Epoch 6 Step 2575 Validation Loss: 1.3898121118545532\n",
            "Epoch 6 Step 2600 Validation Loss: 1.3898264169692993\n",
            "Epoch 6 Step 2625 Validation Loss: 1.3879446983337402\n",
            "Epoch 6 Step 2650 Validation Loss: 1.3845336437225342\n",
            "Epoch 6 Step 2675 Validation Loss: 1.383316159248352\n",
            "Epoch 6 Step 2700 Validation Loss: 1.3852818012237549\n",
            "Epoch 6 Step 2725 Validation Loss: 1.3799059391021729\n",
            "Epoch 6 Step 2750 Validation Loss: 1.3834713697433472\n",
            "Epoch 6 Step 2775 Validation Loss: 1.3796868324279785\n",
            "Epoch 6 Step 2800 Validation Loss: 1.378274917602539\n",
            "Epoch 6 Step 2825 Validation Loss: 1.3850890398025513\n",
            "Epoch 6 Step 2850 Validation Loss: 1.3833119869232178\n",
            "Epoch 6 Step 2875 Validation Loss: 1.3818750381469727\n",
            "Epoch 6 Step 2900 Validation Loss: 1.3760977983474731\n",
            "Epoch 6 Step 2925 Validation Loss: 1.3688631057739258\n",
            "Epoch 7 Step 2950 Validation Loss: 1.371063470840454\n",
            "Epoch 7 Step 2975 Validation Loss: 1.3707042932510376\n",
            "Epoch 7 Step 3000 Validation Loss: 1.3729742765426636\n",
            "Epoch 7 Step 3025 Validation Loss: 1.369902491569519\n",
            "Epoch 7 Step 3050 Validation Loss: 1.3716022968292236\n",
            "Epoch 7 Step 3075 Validation Loss: 1.3643951416015625\n",
            "Epoch 7 Step 3100 Validation Loss: 1.3678761720657349\n",
            "Epoch 7 Step 3125 Validation Loss: 1.360710620880127\n",
            "Epoch 7 Step 3150 Validation Loss: 1.3645498752593994\n",
            "Epoch 7 Step 3175 Validation Loss: 1.3583333492279053\n",
            "Epoch 7 Step 3200 Validation Loss: 1.359556794166565\n",
            "Epoch 7 Step 3225 Validation Loss: 1.3611127138137817\n",
            "Epoch 7 Step 3250 Validation Loss: 1.3578871488571167\n",
            "Epoch 7 Step 3275 Validation Loss: 1.3579212427139282\n",
            "Epoch 7 Step 3300 Validation Loss: 1.3627299070358276\n",
            "Epoch 7 Step 3325 Validation Loss: 1.36642324924469\n",
            "Epoch 7 Step 3350 Validation Loss: 1.366373062133789\n",
            "Epoch 7 Step 3375 Validation Loss: 1.3588507175445557\n",
            "Epoch 7 Step 3400 Validation Loss: 1.355589509010315\n",
            "Epoch 7 Step 3425 Validation Loss: 1.35563325881958\n",
            "Epoch 8 Step 3450 Validation Loss: 1.357016921043396\n",
            "Epoch 8 Step 3475 Validation Loss: 1.3590855598449707\n",
            "Epoch 8 Step 3500 Validation Loss: 1.3526570796966553\n",
            "Epoch 8 Step 3525 Validation Loss: 1.3515607118606567\n",
            "Epoch 8 Step 3550 Validation Loss: 1.3540841341018677\n",
            "Epoch 8 Step 3575 Validation Loss: 1.3501849174499512\n",
            "Epoch 8 Step 3600 Validation Loss: 1.3479013442993164\n",
            "Epoch 8 Step 3625 Validation Loss: 1.3430780172348022\n",
            "Epoch 8 Step 3650 Validation Loss: 1.343811273574829\n",
            "Epoch 8 Step 3675 Validation Loss: 1.341354250907898\n",
            "Epoch 8 Step 3700 Validation Loss: 1.3419759273529053\n",
            "Epoch 8 Step 3725 Validation Loss: 1.3408548831939697\n",
            "Epoch 8 Step 3750 Validation Loss: 1.3432857990264893\n",
            "Epoch 8 Step 3775 Validation Loss: 1.3478000164031982\n",
            "Epoch 8 Step 3800 Validation Loss: 1.3462750911712646\n",
            "Epoch 8 Step 3825 Validation Loss: 1.3459677696228027\n",
            "Epoch 8 Step 3850 Validation Loss: 1.3477166891098022\n",
            "Epoch 8 Step 3875 Validation Loss: 1.3427139520645142\n",
            "Epoch 8 Step 3900 Validation Loss: 1.3436319828033447\n",
            "Epoch 9 Step 3925 Validation Loss: 1.3516318798065186\n",
            "Epoch 9 Step 3950 Validation Loss: 1.3458726406097412\n",
            "Epoch 9 Step 3975 Validation Loss: 1.3428337574005127\n",
            "Epoch 9 Step 4000 Validation Loss: 1.3400280475616455\n",
            "Epoch 9 Step 4025 Validation Loss: 1.3451509475708008\n",
            "Epoch 9 Step 4050 Validation Loss: 1.3404115438461304\n",
            "Epoch 9 Step 4075 Validation Loss: 1.3440520763397217\n",
            "Epoch 9 Step 4100 Validation Loss: 1.336958646774292\n",
            "Epoch 9 Step 4125 Validation Loss: 1.3379303216934204\n",
            "Epoch 9 Step 4150 Validation Loss: 1.336565375328064\n",
            "Epoch 9 Step 4175 Validation Loss: 1.339208960533142\n",
            "Epoch 9 Step 4200 Validation Loss: 1.3361362218856812\n",
            "Epoch 9 Step 4225 Validation Loss: 1.3343015909194946\n",
            "Epoch 9 Step 4250 Validation Loss: 1.335503339767456\n",
            "Epoch 9 Step 4275 Validation Loss: 1.3419125080108643\n",
            "Epoch 9 Step 4300 Validation Loss: 1.343143343925476\n",
            "Epoch 9 Step 4325 Validation Loss: 1.3409085273742676\n",
            "Epoch 9 Step 4350 Validation Loss: 1.3417059183120728\n",
            "Epoch 9 Step 4375 Validation Loss: 1.33953058719635\n",
            "Epoch 9 Step 4400 Validation Loss: 1.335315465927124\n",
            "Epoch 10 Step 4425 Validation Loss: 1.3381268978118896\n",
            "Epoch 10 Step 4450 Validation Loss: 1.335768699645996\n",
            "Epoch 10 Step 4475 Validation Loss: 1.3319913148880005\n",
            "Epoch 10 Step 4500 Validation Loss: 1.3354933261871338\n",
            "Epoch 10 Step 4525 Validation Loss: 1.3346794843673706\n",
            "Epoch 10 Step 4550 Validation Loss: 1.333021640777588\n",
            "Epoch 10 Step 4575 Validation Loss: 1.3354588747024536\n",
            "Epoch 10 Step 4600 Validation Loss: 1.3319774866104126\n",
            "Epoch 10 Step 4625 Validation Loss: 1.33524751663208\n",
            "Epoch 10 Step 4650 Validation Loss: 1.3279248476028442\n",
            "Epoch 10 Step 4675 Validation Loss: 1.32480788230896\n",
            "Epoch 10 Step 4700 Validation Loss: 1.3295972347259521\n",
            "Epoch 10 Step 4725 Validation Loss: 1.3262081146240234\n",
            "Epoch 10 Step 4750 Validation Loss: 1.3256514072418213\n",
            "Epoch 10 Step 4775 Validation Loss: 1.3292043209075928\n",
            "Epoch 10 Step 4800 Validation Loss: 1.3317265510559082\n",
            "Epoch 10 Step 4825 Validation Loss: 1.3334521055221558\n",
            "Epoch 10 Step 4850 Validation Loss: 1.328114628791809\n",
            "Epoch 10 Step 4875 Validation Loss: 1.3281350135803223\n",
            "Epoch 10 Step 4900 Validation Loss: 1.3312729597091675\n",
            "Epoch 11 Step 4925 Validation Loss: 1.3368529081344604\n",
            "Epoch 11 Step 4950 Validation Loss: 1.3344377279281616\n",
            "Epoch 11 Step 4975 Validation Loss: 1.3311386108398438\n",
            "Epoch 11 Step 5000 Validation Loss: 1.3278096914291382\n",
            "Epoch 11 Step 5025 Validation Loss: 1.3272459506988525\n",
            "Epoch 11 Step 5050 Validation Loss: 1.320286750793457\n",
            "Epoch 11 Step 5075 Validation Loss: 1.3247878551483154\n",
            "Epoch 11 Step 5100 Validation Loss: 1.3227485418319702\n",
            "Epoch 11 Step 5125 Validation Loss: 1.324934482574463\n",
            "Epoch 11 Step 5150 Validation Loss: 1.3222588300704956\n",
            "Epoch 11 Step 5175 Validation Loss: 1.3239459991455078\n",
            "Epoch 11 Step 5200 Validation Loss: 1.3211742639541626\n",
            "Epoch 11 Step 5225 Validation Loss: 1.3217203617095947\n",
            "Epoch 11 Step 5250 Validation Loss: 1.3261843919754028\n",
            "Epoch 11 Step 5275 Validation Loss: 1.3233782052993774\n",
            "Epoch 11 Step 5300 Validation Loss: 1.3223820924758911\n",
            "Epoch 11 Step 5325 Validation Loss: 1.3272837400436401\n",
            "Epoch 11 Step 5350 Validation Loss: 1.3226429224014282\n",
            "Epoch 11 Step 5375 Validation Loss: 1.324352741241455\n",
            "Epoch 12 Step 5400 Validation Loss: 1.3258615732192993\n",
            "Epoch 12 Step 5425 Validation Loss: 1.325698971748352\n",
            "Epoch 12 Step 5450 Validation Loss: 1.3268927335739136\n",
            "Epoch 12 Step 5475 Validation Loss: 1.3225017786026\n",
            "Epoch 12 Step 5500 Validation Loss: 1.326035499572754\n",
            "Epoch 12 Step 5525 Validation Loss: 1.3195054531097412\n",
            "Epoch 12 Step 5550 Validation Loss: 1.3223025798797607\n",
            "Epoch 12 Step 5575 Validation Loss: 1.3141164779663086\n",
            "Epoch 12 Step 5600 Validation Loss: 1.3171021938323975\n",
            "Epoch 12 Step 5625 Validation Loss: 1.3150569200515747\n",
            "Epoch 12 Step 5650 Validation Loss: 1.3127604722976685\n",
            "Epoch 12 Step 5675 Validation Loss: 1.316654086112976\n",
            "Epoch 12 Step 5700 Validation Loss: 1.3160293102264404\n",
            "Epoch 12 Step 5725 Validation Loss: 1.3163808584213257\n",
            "Epoch 12 Step 5750 Validation Loss: 1.3229058980941772\n",
            "Epoch 12 Step 5775 Validation Loss: 1.3226439952850342\n",
            "Epoch 12 Step 5800 Validation Loss: 1.3253700733184814\n",
            "Epoch 12 Step 5825 Validation Loss: 1.3191888332366943\n",
            "Epoch 12 Step 5850 Validation Loss: 1.3214658498764038\n",
            "Epoch 12 Step 5875 Validation Loss: 1.3190743923187256\n",
            "Epoch 13 Step 5900 Validation Loss: 1.3218175172805786\n",
            "Epoch 13 Step 5925 Validation Loss: 1.3284474611282349\n",
            "Epoch 13 Step 5950 Validation Loss: 1.3219870328903198\n",
            "Epoch 13 Step 5975 Validation Loss: 1.317441463470459\n",
            "Epoch 13 Step 6000 Validation Loss: 1.3177895545959473\n",
            "Epoch 13 Step 6025 Validation Loss: 1.314903736114502\n",
            "Epoch 13 Step 6050 Validation Loss: 1.3141319751739502\n",
            "Epoch 13 Step 6075 Validation Loss: 1.3097693920135498\n",
            "Epoch 13 Step 6100 Validation Loss: 1.3160518407821655\n",
            "Epoch 13 Step 6125 Validation Loss: 1.316007375717163\n",
            "Epoch 13 Step 6150 Validation Loss: 1.3118386268615723\n",
            "Epoch 13 Step 6175 Validation Loss: 1.3074753284454346\n",
            "Epoch 13 Step 6200 Validation Loss: 1.3126158714294434\n",
            "Epoch 13 Step 6225 Validation Loss: 1.3127483129501343\n",
            "Epoch 13 Step 6250 Validation Loss: 1.3167507648468018\n",
            "Epoch 13 Step 6275 Validation Loss: 1.3160309791564941\n",
            "Epoch 13 Step 6300 Validation Loss: 1.3182276487350464\n",
            "Epoch 13 Step 6325 Validation Loss: 1.3160204887390137\n",
            "Epoch 13 Step 6350 Validation Loss: 1.3156065940856934\n",
            "Epoch 14 Step 6375 Validation Loss: 1.3227914571762085\n",
            "Epoch 14 Step 6400 Validation Loss: 1.3193471431732178\n",
            "Epoch 14 Step 6425 Validation Loss: 1.3119488954544067\n",
            "Epoch 14 Step 6450 Validation Loss: 1.3215391635894775\n",
            "Epoch 14 Step 6475 Validation Loss: 1.318572759628296\n",
            "Epoch 14 Step 6500 Validation Loss: 1.3074748516082764\n",
            "Epoch 14 Step 6525 Validation Loss: 1.3131678104400635\n",
            "Epoch 14 Step 6550 Validation Loss: 1.3145687580108643\n",
            "Epoch 14 Step 6575 Validation Loss: 1.316894292831421\n",
            "Epoch 14 Step 6600 Validation Loss: 1.3109991550445557\n",
            "Epoch 14 Step 6625 Validation Loss: 1.312558889389038\n",
            "Epoch 14 Step 6650 Validation Loss: 1.3141846656799316\n",
            "Epoch 14 Step 6675 Validation Loss: 1.3118332624435425\n",
            "Epoch 14 Step 6700 Validation Loss: 1.3106497526168823\n",
            "Epoch 14 Step 6725 Validation Loss: 1.3193011283874512\n",
            "Epoch 14 Step 6750 Validation Loss: 1.3130077123641968\n",
            "Epoch 14 Step 6775 Validation Loss: 1.3165780305862427\n",
            "Epoch 14 Step 6800 Validation Loss: 1.3158113956451416\n",
            "Epoch 14 Step 6825 Validation Loss: 1.314945101737976\n",
            "Epoch 14 Step 6850 Validation Loss: 1.3197135925292969\n",
            "Epoch 15 Step 6875 Validation Loss: 1.3149176836013794\n",
            "Epoch 15 Step 6900 Validation Loss: 1.314357042312622\n",
            "Epoch 15 Step 6925 Validation Loss: 1.3150324821472168\n",
            "Epoch 15 Step 6950 Validation Loss: 1.3145036697387695\n",
            "Epoch 15 Step 6975 Validation Loss: 1.3123637437820435\n",
            "Epoch 15 Step 7000 Validation Loss: 1.3091214895248413\n",
            "Epoch 15 Step 7025 Validation Loss: 1.312030553817749\n",
            "Epoch 15 Step 7050 Validation Loss: 1.3055200576782227\n",
            "Epoch 15 Step 7075 Validation Loss: 1.3096401691436768\n",
            "Epoch 15 Step 7100 Validation Loss: 1.303739070892334\n",
            "Epoch 15 Step 7125 Validation Loss: 1.3057336807250977\n",
            "Epoch 15 Step 7150 Validation Loss: 1.3032467365264893\n",
            "Epoch 15 Step 7175 Validation Loss: 1.3012501001358032\n",
            "Epoch 15 Step 7200 Validation Loss: 1.3079090118408203\n",
            "Epoch 15 Step 7225 Validation Loss: 1.3103522062301636\n",
            "Epoch 15 Step 7250 Validation Loss: 1.3114784955978394\n",
            "Epoch 15 Step 7275 Validation Loss: 1.312544822692871\n",
            "Epoch 15 Step 7300 Validation Loss: 1.3082795143127441\n",
            "Epoch 15 Step 7325 Validation Loss: 1.308674693107605\n",
            "Epoch 15 Step 7350 Validation Loss: 1.3130650520324707\n",
            "Epoch 16 Step 7375 Validation Loss: 1.3062132596969604\n",
            "Epoch 16 Step 7400 Validation Loss: 1.3182623386383057\n",
            "Epoch 16 Step 7425 Validation Loss: 1.312393307685852\n",
            "Epoch 16 Step 7450 Validation Loss: 1.315233826637268\n",
            "Epoch 16 Step 7475 Validation Loss: 1.3085389137268066\n",
            "Epoch 16 Step 7500 Validation Loss: 1.3041024208068848\n",
            "Epoch 16 Step 7525 Validation Loss: 1.3036144971847534\n",
            "Epoch 16 Step 7550 Validation Loss: 1.3033974170684814\n",
            "Epoch 16 Step 7575 Validation Loss: 1.3083555698394775\n",
            "Epoch 16 Step 7600 Validation Loss: 1.308108925819397\n",
            "Epoch 16 Step 7625 Validation Loss: 1.3102694749832153\n",
            "Epoch 16 Step 7650 Validation Loss: 1.3063812255859375\n",
            "Epoch 16 Step 7675 Validation Loss: 1.300487995147705\n",
            "Epoch 16 Step 7700 Validation Loss: 1.3115079402923584\n",
            "Epoch 16 Step 7725 Validation Loss: 1.305847406387329\n",
            "Epoch 16 Step 7750 Validation Loss: 1.3079144954681396\n",
            "Epoch 16 Step 7775 Validation Loss: 1.311020851135254\n",
            "Epoch 16 Step 7800 Validation Loss: 1.3039902448654175\n",
            "Epoch 16 Step 7825 Validation Loss: 1.3089760541915894\n",
            "Epoch 17 Step 7850 Validation Loss: 1.3110061883926392\n",
            "Epoch 17 Step 7875 Validation Loss: 1.3058674335479736\n",
            "Epoch 17 Step 7900 Validation Loss: 1.3083387613296509\n",
            "Epoch 17 Step 7925 Validation Loss: 1.3062106370925903\n",
            "Epoch 17 Step 7950 Validation Loss: 1.3076717853546143\n",
            "Epoch 17 Step 7975 Validation Loss: 1.3002370595932007\n",
            "Epoch 17 Step 8000 Validation Loss: 1.3126370906829834\n",
            "Epoch 17 Step 8025 Validation Loss: 1.30745530128479\n",
            "Epoch 17 Step 8050 Validation Loss: 1.3082083463668823\n",
            "Epoch 17 Step 8075 Validation Loss: 1.3074575662612915\n",
            "Epoch 17 Step 8100 Validation Loss: 1.3003761768341064\n",
            "Epoch 17 Step 8125 Validation Loss: 1.304528832435608\n",
            "Epoch 17 Step 8150 Validation Loss: 1.3033148050308228\n",
            "Epoch 17 Step 8175 Validation Loss: 1.2992124557495117\n",
            "Epoch 17 Step 8200 Validation Loss: 1.3073176145553589\n",
            "Epoch 17 Step 8225 Validation Loss: 1.3100968599319458\n",
            "Epoch 17 Step 8250 Validation Loss: 1.3134393692016602\n",
            "Epoch 17 Step 8275 Validation Loss: 1.3098933696746826\n",
            "Epoch 17 Step 8300 Validation Loss: 1.3104907274246216\n",
            "Epoch 17 Step 8325 Validation Loss: 1.3068733215332031\n",
            "Epoch 18 Step 8350 Validation Loss: 1.3042442798614502\n",
            "Epoch 18 Step 8375 Validation Loss: 1.3104273080825806\n",
            "Epoch 18 Step 8400 Validation Loss: 1.3075013160705566\n",
            "Epoch 18 Step 8425 Validation Loss: 1.3020604848861694\n",
            "Epoch 18 Step 8450 Validation Loss: 1.3053512573242188\n",
            "Epoch 18 Step 8475 Validation Loss: 1.2960877418518066\n",
            "Epoch 18 Step 8500 Validation Loss: 1.3041174411773682\n",
            "Epoch 18 Step 8525 Validation Loss: 1.3018792867660522\n",
            "Epoch 18 Step 8550 Validation Loss: 1.3051694631576538\n",
            "Epoch 18 Step 8575 Validation Loss: 1.300957202911377\n",
            "Epoch 18 Step 8600 Validation Loss: 1.300222635269165\n",
            "Epoch 18 Step 8625 Validation Loss: 1.299349069595337\n",
            "Epoch 18 Step 8650 Validation Loss: 1.29539155960083\n",
            "Epoch 18 Step 8675 Validation Loss: 1.3033584356307983\n",
            "Epoch 18 Step 8700 Validation Loss: 1.3038054704666138\n",
            "Epoch 18 Step 8725 Validation Loss: 1.300610899925232\n",
            "Epoch 18 Step 8750 Validation Loss: 1.3022198677062988\n",
            "Epoch 18 Step 8775 Validation Loss: 1.3001302480697632\n",
            "Epoch 18 Step 8800 Validation Loss: 1.3040181398391724\n",
            "Epoch 19 Step 8825 Validation Loss: 1.3063592910766602\n",
            "Epoch 19 Step 8850 Validation Loss: 1.3027582168579102\n",
            "Epoch 19 Step 8875 Validation Loss: 1.3056623935699463\n",
            "Epoch 19 Step 8900 Validation Loss: 1.3020833730697632\n",
            "Epoch 19 Step 8925 Validation Loss: 1.3036253452301025\n",
            "Epoch 19 Step 8950 Validation Loss: 1.2996699810028076\n",
            "Epoch 19 Step 8975 Validation Loss: 1.3029063940048218\n",
            "Epoch 19 Step 9000 Validation Loss: 1.3017537593841553\n",
            "Epoch 19 Step 9025 Validation Loss: 1.3018639087677002\n",
            "Epoch 19 Step 9050 Validation Loss: 1.301722526550293\n",
            "Epoch 19 Step 9075 Validation Loss: 1.2981722354888916\n",
            "Epoch 19 Step 9100 Validation Loss: 1.2979929447174072\n",
            "Epoch 19 Step 9125 Validation Loss: 1.2957069873809814\n",
            "Epoch 19 Step 9150 Validation Loss: 1.294991374015808\n",
            "Epoch 19 Step 9175 Validation Loss: 1.3017942905426025\n",
            "Epoch 19 Step 9200 Validation Loss: 1.2983427047729492\n",
            "Epoch 19 Step 9225 Validation Loss: 1.3040276765823364\n",
            "Epoch 19 Step 9250 Validation Loss: 1.300875186920166\n",
            "Epoch 19 Step 9275 Validation Loss: 1.2996084690093994\n",
            "Epoch 19 Step 9300 Validation Loss: 1.3023573160171509\n",
            "Epoch 20 Step 9325 Validation Loss: 1.3008687496185303\n",
            "Epoch 20 Step 9350 Validation Loss: 1.3012195825576782\n",
            "Epoch 20 Step 9375 Validation Loss: 1.3008124828338623\n",
            "Epoch 20 Step 9400 Validation Loss: 1.2988519668579102\n",
            "Epoch 20 Step 9425 Validation Loss: 1.3030717372894287\n",
            "Epoch 20 Step 9450 Validation Loss: 1.2982640266418457\n",
            "Epoch 20 Step 9475 Validation Loss: 1.3016681671142578\n",
            "Epoch 20 Step 9500 Validation Loss: 1.299776315689087\n",
            "Epoch 20 Step 9525 Validation Loss: 1.3005543947219849\n",
            "Epoch 20 Step 9550 Validation Loss: 1.3006513118743896\n",
            "Epoch 20 Step 9575 Validation Loss: 1.3002487421035767\n",
            "Epoch 20 Step 9600 Validation Loss: 1.300780177116394\n",
            "Epoch 20 Step 9625 Validation Loss: 1.298556923866272\n",
            "Epoch 20 Step 9650 Validation Loss: 1.3003966808319092\n",
            "Epoch 20 Step 9675 Validation Loss: 1.3059020042419434\n",
            "Epoch 20 Step 9700 Validation Loss: 1.3051848411560059\n",
            "Epoch 20 Step 9725 Validation Loss: 1.3053618669509888\n",
            "Epoch 20 Step 9750 Validation Loss: 1.305740237236023\n",
            "Epoch 20 Step 9775 Validation Loss: 1.3051540851593018\n",
            "Epoch 20 Step 9800 Validation Loss: 1.300707221031189\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "batch_size = 100\n",
        "sequence_len = 100\n",
        "tracker = 0\n",
        "num_char = max(encoded_text)+1\n",
        "\n",
        "model.train()\n",
        "\n",
        "if model.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "    hidden_state = model.hidden_state(batch_size)\n",
        "\n",
        "    for x, y in generate_batches(train_data,batch_size,sequence_len):\n",
        "        tracker += 1\n",
        "        x = one_hot_encode(x,num_char)\n",
        "\n",
        "        inputs = torch.from_numpy(x)\n",
        "        outputs = torch.from_numpy(y)\n",
        "\n",
        "        if model.use_gpu:\n",
        "            inputs = inputs.cuda()\n",
        "            outputs = outputs.cuda()\n",
        "\n",
        "        hidden_state = tuple([state.data for state in hidden_state])\n",
        "        model.zero_grad()\n",
        "\n",
        "        lstm_output, hidden_state = model.forward(inputs,hidden_state)\n",
        "        loss = criterion(lstm_output,outputs.view(batch_size*sequence_len).long())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
        "        optimizer.step()\n",
        "\n",
        "        if tracker % 25 == 0:\n",
        "            val_hidden_state = model.hidden_state(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "\n",
        "            for x, y in generate_batches(test_data,batch_size,sequence_len):\n",
        "                x = one_hot_encode(x,num_char)\n",
        "\n",
        "                inputs = torch.from_numpy(x)\n",
        "                outputs = torch.from_numpy(y)\n",
        "\n",
        "                if model.use_gpu:\n",
        "                    inputs = inputs.cuda()\n",
        "                    outputs = outputs.cuda()\n",
        "\n",
        "                val_hidden_state = tuple([state.data for state in val_hidden_state])\n",
        "\n",
        "                lstm_output, val_hidden_state = model.forward(inputs,val_hidden_state)\n",
        "                val_loss = criterion(lstm_output,outputs.view(batch_size*sequence_len).long())\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            print(f\"Epoch {i+1} Step {tracker} Validation Loss: {val_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "bbf24a01-7453-4b04-a673-0610e4f9110d",
      "metadata": {
        "id": "bbf24a01-7453-4b04-a673-0610e4f9110d"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),\"shakespeare_text_generator.net\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_char(model,char,hidden_state=None,k=1):\n",
        "    encoded_text = model.encoder[char]\n",
        "    encoded_text = np.array([[encoded_text]])\n",
        "    encoded_text = one_hot_encode(encoded_text,len(model.unique_char))\n",
        "    encoded_text = torch.from_numpy(encoded_text)\n",
        "\n",
        "    if model.use_gpu:\n",
        "      encoded_text = encoded_text.cuda()\n",
        "\n",
        "    hidden_state = tuple([state.data for state in hidden_state])\n",
        "    lstm_output, hidden_state = model.forward(encoded_text,hidden_state)\n",
        "    probs = F.softmax(lstm_output,dim=1).data # Raw probabilities for every single character\n",
        "\n",
        "    if model.use_gpu:\n",
        "        probs = probs.cpu() # Use probabilities with Numpy to get top K probabilities\n",
        "\n",
        "    probs, index_positions = probs.topk(k) # Returns how many characters to consider in our probabilities choice\n",
        "    index_positions = index_positions.numpy().squeeze() # Put it back into the CPU and get it into the right shape\n",
        "    probs = probs.numpy().flatten() # Convert to probabilities per index\n",
        "    probs = probs/probs.sum()\n",
        "    char = np.random.choice(index_positions,p=probs)\n",
        "    return model.decoder[char], hidden_state"
      ],
      "metadata": {
        "id": "n7LHTQGwHIcM"
      },
      "id": "n7LHTQGwHIcM",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model,size,seed=\"The\",k=1):\n",
        "    if model.use_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    model.eval()\n",
        "    output_chars = [char for char in seed]\n",
        "    hidden_state = model.hidden_state(1)\n",
        "\n",
        "    for char in seed:\n",
        "      char, hidden_state = predict_next_char(model,char,hidden_state,k)\n",
        "\n",
        "    output_chars.append(char)\n",
        "\n",
        "    for i in range(size):\n",
        "      char, hidden_state = predict_next_char(model,output_chars[-1],hidden_state,k)\n",
        "      output_chars.append(char)\n",
        "\n",
        "    return \"\".join(output_chars)"
      ],
      "metadata": {
        "id": "vlQeouJHI-_Q"
      },
      "id": "vlQeouJHI-_Q",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model,size=1000,seed=\"The \",k=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCmhWNAKJl5f",
        "outputId": "2f7e5d17-7c2f-49ac-93f6-8eb138e13720"
      },
      "id": "mCmhWNAKJl5f",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The states.\n",
            "\n",
            "                              Enter CLARENCE\n",
            "\n",
            "    There's a service to you.\n",
            "  CASSIUS. I will be so so so much and shed.\n",
            "    I will not strike the will and more than she.\n",
            "    I would not be a most assurance to my son.\n",
            "    If they should spend myself it were a strange,\n",
            "    Whose such an angel to an angel show\n",
            "    With such a man off as I will not stay,\n",
            "    And we are all the matter of his fair\n",
            "    To merciless the state. I seek more son\n",
            "    The strong of men that the with this distain  \n",
            "    With the secret of my soldiers that the most  \n",
            "    They say I shall stay where you are a serving\n",
            "    And she it was the sun and men of heaven,\n",
            "    With strength to she with me and wear the war.\n",
            "    I would not set them well. Then I have strong,\n",
            "    To tell you, and I'll tell you that I have\n",
            "    I should be sent to th' traitor. Thou shalt not be.\n",
            "    I will not see the common thoughts of her\n",
            "    That the most soul of men's dear singer thought\n",
            "    That sent his heart as words, and which in him\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}